{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deeksha2107/temp/blob/main/docs/2notebook/Example_1_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7MeFJtNalS2"
      },
      "source": [
        "## sklearn and TextAttack\n",
        "\n",
        "This following code trains two different text classification models using sklearn. Both use logistic regression models: the difference is in the features.\n",
        "\n",
        "We will load data using `datasets`, train the models, and attack them using TextAttack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFR3zFvBalS3"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QData/TextAttack/blob/master/docs/2notebook/Example_1_sklearn.ipynb)\n",
        "\n",
        "[![View Source on GitHub](https://img.shields.io/badge/github-view%20source-black.svg)](https://github.com/QData/TextAttack/blob/master/docs/2notebook/Example_1_sklearn.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl0ecB41E75i",
        "outputId": "aaf0478d-b053-4ef8-e22a-b7da9834ab55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWh7ZuC3alS4"
      },
      "source": [
        "Please remember to run  **pip3 install textattack[tensorflow]**  in your notebook enviroment before the following codes:\n",
        "\n",
        "### Training\n",
        "\n",
        "This code trains two models: one on bag-of-words statistics (`bow_unstemmed`) and one on tf–idf statistics (`tfidf_unstemmed`). The dataset is the IMDB movie review dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJrB3uszalS4",
        "outputId": "e7ca52f5-cb07-49c3-d124-4470deaa4a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import nltk  # the Natural Language Toolkit\n",
        "\n",
        "nltk.download(\"punkt\")  # The NLTK tokenizer\n",
        "nltk.download(\"stopwords\")  # Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E3gGgXfalS5",
        "outputId": "22cc044d-f58a-46b3-d3ee-c91858530f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...successfully loaded training data\n",
            "Total length of training data:  8530\n",
            "...augmented data with len_tokens and average_words\n",
            "...successfully loaded testing data\n",
            "Total length of testing data:  1066\n",
            "...augmented data with len_tokens and average_words\n",
            "...successfully created the unstemmed BOW data\n",
            "...successfully created the unstemmed TFIDF data\n",
            "Training accuracy of BOW Unstemmed:  0.6228604923798359\n",
            "Testing accuracy of BOW Unstemmed:  0.6060037523452158\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.69      0.64       533\n",
            "           1       0.63      0.52      0.57       533\n",
            "\n",
            "    accuracy                           0.61      1066\n",
            "   macro avg       0.61      0.61      0.60      1066\n",
            "weighted avg       0.61      0.61      0.60      1066\n",
            "\n",
            "Training accuracy of TFIDF Unstemmed:  0.6227432590855803\n",
            "Testing accuracy of TFIDF Unstemmed:  0.6078799249530957\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.68      0.63       533\n",
            "           1       0.62      0.54      0.58       533\n",
            "\n",
            "    accuracy                           0.61      1066\n",
            "   macro avg       0.61      0.61      0.61      1066\n",
            "weighted avg       0.61      0.61      0.61      1066\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Nice to see additional metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def load_data(dataset_split=\"train\"):\n",
        "    dataset = datasets.load_dataset(\"rotten_tomatoes\")[dataset_split]\n",
        "    # Open and import positve data\n",
        "    df = pd.DataFrame()\n",
        "    df[\"Review\"] = [review[\"text\"] for review in dataset]\n",
        "    df[\"Sentiment\"] = [review[\"label\"] for review in dataset]\n",
        "    # Remove non-alphanumeric characters\n",
        "    df[\"Review\"] = df[\"Review\"].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", str(x)))\n",
        "    # Tokenize the training and testing data\n",
        "    df_tokenized = tokenize_review(df)\n",
        "    return df_tokenized\n",
        "\n",
        "\n",
        "def tokenize_review(df):\n",
        "    # Tokenize Reviews in training\n",
        "    tokened_reviews = [word_tokenize(rev) for rev in df[\"Review\"]]\n",
        "    # Create word stems\n",
        "    stemmed_tokens = []\n",
        "    porter = PorterStemmer()\n",
        "    for i in range(len(tokened_reviews)):\n",
        "        stems = [porter.stem(token) for token in tokened_reviews[i]]\n",
        "        stems = \" \".join(stems)\n",
        "        stemmed_tokens.append(stems)\n",
        "    df.insert(1, column=\"Stemmed\", value=stemmed_tokens)\n",
        "    return df\n",
        "\n",
        "\n",
        "def transform_BOW(training, testing, column_name):\n",
        "    vect = CountVectorizer(\n",
        "        max_features=100, ngram_range=(1, 3), stop_words=\"english\"\n",
        "    )\n",
        "    vectFit = vect.fit(training[column_name])\n",
        "    BOW_training = vectFit.transform(training[column_name])\n",
        "    BOW_training_df = pd.DataFrame(\n",
        "        BOW_training.toarray(), columns=vect.get_feature_names_out()\n",
        "    )\n",
        "    BOW_testing = vectFit.transform(testing[column_name])\n",
        "    BOW_testing_Df = pd.DataFrame(\n",
        "        BOW_testing.toarray(), columns=vect.get_feature_names_out()\n",
        "    )\n",
        "    return vectFit, BOW_training_df, BOW_testing_Df\n",
        "\n",
        "\n",
        "def transform_tfidf(training, testing, column_name):\n",
        "    Tfidf = TfidfVectorizer(\n",
        "        ngram_range=(1, 3), max_features=100, stop_words=\"english\"\n",
        "    )\n",
        "    Tfidf_fit = Tfidf.fit(training[column_name])\n",
        "    Tfidf_training = Tfidf_fit.transform(training[column_name])\n",
        "    Tfidf_training_df = pd.DataFrame(\n",
        "        Tfidf_training.toarray(), columns=Tfidf.get_feature_names_out()\n",
        "    )\n",
        "    Tfidf_testing = Tfidf_fit.transform(testing[column_name])\n",
        "    Tfidf_testing_df = pd.DataFrame(\n",
        "        Tfidf_testing.toarray(), columns=Tfidf.get_feature_names_out()\n",
        "    )\n",
        "    return Tfidf_fit, Tfidf_training_df, Tfidf_testing_df\n",
        "\n",
        "\n",
        "def add_augmenting_features(df):\n",
        "    tokened_reviews = [word_tokenize(rev) for rev in df[\"Review\"]]\n",
        "    # Create feature that measures length of reviews\n",
        "    len_tokens = []\n",
        "    for i in range(len(tokened_reviews)):\n",
        "        len_tokens.append(len(tokened_reviews[i]))\n",
        "    len_tokens = preprocessing.scale(len_tokens)\n",
        "    df.insert(0, column=\"Lengths\", value=len_tokens)\n",
        "\n",
        "    # Create average word length (training)\n",
        "    Average_Words = [len(x) / (len(x.split())) for x in df[\"Review\"].tolist()]\n",
        "    Average_Words = preprocessing.scale(Average_Words)\n",
        "    df[\"averageWords\"] = Average_Words\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_model(X_train, y_train, X_test, y_test, name_of_test):\n",
        "    log_reg = LogisticRegression(C=30, max_iter=200).fit(X_train, y_train)\n",
        "    y_pred = log_reg.predict(X_test)\n",
        "    print(\n",
        "        \"Training accuracy of \" + name_of_test + \": \", log_reg.score(X_train, y_train)\n",
        "    )\n",
        "    print(\"Testing accuracy of \" + name_of_test + \": \", log_reg.score(X_test, y_test))\n",
        "    print(classification_report(y_test, y_pred))  # Evaluating prediction ability\n",
        "    return log_reg\n",
        "\n",
        "\n",
        "# Load training and test sets\n",
        "# Loading reviews into DF\n",
        "df_train = load_data(\"train\")\n",
        "\n",
        "print(\"...successfully loaded training data\")\n",
        "print(\"Total length of training data: \", len(df_train))\n",
        "# Add augmenting features\n",
        "df_train = add_augmenting_features(df_train)\n",
        "print(\"...augmented data with len_tokens and average_words\")\n",
        "\n",
        "# Load test DF\n",
        "df_test = load_data(\"test\")\n",
        "\n",
        "print(\"...successfully loaded testing data\")\n",
        "print(\"Total length of testing data: \", len(df_test))\n",
        "df_test = add_augmenting_features(df_test)\n",
        "print(\"...augmented data with len_tokens and average_words\")\n",
        "\n",
        "# Create unstemmed BOW features for training set\n",
        "unstemmed_BOW_vect_fit, df_train_bow_unstem, df_test_bow_unstem = transform_BOW(\n",
        "    df_train, df_test, \"Review\"\n",
        ")\n",
        "print(\"...successfully created the unstemmed BOW data\")\n",
        "\n",
        "# Create TfIdf features for training set\n",
        "unstemmed_tfidf_vect_fit, df_train_tfidf_unstem, df_test_tfidf_unstem = transform_tfidf(\n",
        "    df_train, df_test, \"Review\"\n",
        ")\n",
        "print(\"...successfully created the unstemmed TFIDF data\")\n",
        "\n",
        "# Running logistic regression on dataframes\n",
        "bow_unstemmed = build_model(\n",
        "    df_train_bow_unstem,\n",
        "    df_train[\"Sentiment\"],\n",
        "    df_test_bow_unstem,\n",
        "    df_test[\"Sentiment\"],\n",
        "    \"BOW Unstemmed\",\n",
        ")\n",
        "\n",
        "tfidf_unstemmed = build_model(\n",
        "    df_train_tfidf_unstem,\n",
        "    df_train[\"Sentiment\"],\n",
        "    df_test_tfidf_unstem,\n",
        "    df_test[\"Sentiment\"],\n",
        "    \"TFIDF Unstemmed\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_T3Q5vralS6"
      },
      "source": [
        "### Attacking\n",
        "\n",
        "TextAttack includes a build-in `SklearnModelWrapper` that can run attacks on most sklearn models. (If your tokenization strategy is different than above, you may need to subclass `SklearnModelWrapper` to make sure the model inputs & outputs come in the correct format.)\n",
        "\n",
        "Once we initializes the model wrapper, we load a few samples from the IMDB dataset and run the `TextFoolerJin2019` attack on our model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textattack\n",
        "import textattack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c0d28dbb43534b14bd0266d066ef4c37",
            "d505aaaeaf3e48ae89b34af78f3a8aa8",
            "4727a90f5f774c5f8ee2ea5fa8b5264c",
            "32350fc818ee4c2faee527102b1323b7",
            "d86569714c18449084caac742ad93e53",
            "48dce1ef9f9d43a3b35bd6b084bcab78",
            "ea0f7d2cb5884284b25ba77da9faf28c",
            "f58e7362e45940459ca2be4bb7c63035",
            "62ad3bf5c77a4545b55c34f9296b0fb5",
            "187e7c2335b94e1f9af230cef74798e9",
            "064cdedbfd5944ffa2cf5a2fcfbc92ce"
          ]
        },
        "id": "wnT7dvDpbYC3",
        "outputId": "cdad280b-0da6-4acf-a746-2ce3b75e761a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textattack\n",
            "  Downloading textattack-0.3.10-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting bert-score>=0.3.5 (from textattack)\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack) (0.8.1)\n",
            "Collecting flair (from textattack)\n",
            "  Downloading flair-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack) (3.16.1)\n",
            "Collecting language-tool-python (from textattack)\n",
            "  Downloading language_tool_python-2.8.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting lemminflect (from textattack)\n",
            "  Downloading lemminflect-0.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting lru-dict (from textattack)\n",
            "  Downloading lru_dict-1.3.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (3.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.13.1)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (4.44.2)\n",
            "Collecting terminaltables (from textattack)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack) (4.66.6)\n",
            "Collecting word2number (from textattack)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting num2words (from textattack)\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack) (10.5.0)\n",
            "Collecting pinyin>=0.4.0 (from textattack)\n",
            "  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack) (0.42.1)\n",
            "Collecting OpenHowNet (from textattack)\n",
            "  Downloading OpenHowNet-2.0-py3-none-any.whl.metadata (821 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (24.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.4.0->textattack) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2024.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.19.1)\n",
            "Collecting boto3>=1.20.27 (from flair->textattack)\n",
            "  Downloading boto3-1.35.58-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting conllu<5.0.0,>=4.0 (from flair->textattack)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.14)\n",
            "Collecting ftfy>=6.1.0 (from flair->textattack)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (5.2.0)\n",
            "Collecting langdetect>=1.0.9 (from flair->textattack)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (5.3.0)\n",
            "Collecting mpld3>=0.3 (from flair->textattack)\n",
            "  Downloading mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pptree>=3.1 (from flair->textattack)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair->textattack)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.5.2)\n",
            "Collecting segtok>=1.5.11 (from flair->textattack)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair->textattack)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.9.0)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair->textattack)\n",
            "  Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting wikipedia-api>=0.5.7 (from flair->textattack)\n",
            "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting semver<4.0.0,>=3.0.0 (from flair->textattack)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting bioc<3.0.0,>=2.0.0 (from flair->textattack)\n",
            "  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from language-tool-python->textattack) (24.1.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from language-tool-python->textattack) (0.44.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (1.4.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->textattack)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting anytree (from OpenHowNet->textattack)\n",
            "  Downloading anytree-2.12.1-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (75.1.0)\n",
            "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair->textattack)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting intervaltree (from bioc<3.0.0,>=2.0.0->flair->textattack)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.36.0,>=1.35.58 (from boto3>=1.20.27->flair->textattack)\n",
            "  Downloading botocore-1.35.58-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair->textattack)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair->textattack)\n",
            "  Downloading s3transfer-0.10.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair->textattack) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (4.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair->textattack) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (4.12.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect>=1.0.9->flair->textattack) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (2024.8.30)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair->textattack) (3.5.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair->textattack) (3.20.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair->textattack) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (3.0.2)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair->textattack) (0.34.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.4.0->textattack) (0.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.6)\n",
            "Collecting sortedcontainers<3.0,>=2.0 (from intervaltree->bioc<3.0.0,>=2.0.0->flair->textattack)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.4.0->flair->textattack) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair->textattack) (5.9.5)\n",
            "Downloading textattack-0.3.10-py3-none-any.whl (445 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.7/445.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flair-0.14.0-py3-none-any.whl (776 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.5/776.5 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading language_tool_python-2.8.1-py3-none-any.whl (35 kB)\n",
            "Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lru_dict-1.3.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
            "Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Downloading bioc-2.1-py3-none-any.whl (33 kB)\n",
            "Downloading boto3-1.35.58-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.58-py3-none-any.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: pinyin, word2number, docopt, langdetect, pptree, sqlitedict, wikipedia-api, intervaltree\n",
            "  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630477 sha256=bdabc84285bae7a712d6d92606996334224711647b9b434fa8ac2a8817681a0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/38/af/616fc6f154aa5bae65a1da12b22d79943434269f0468ff9b3f\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=fa6bad8338d853fc35e1139f1dfea15b30b85ae3bb29d4590fb89fa4b4680b9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=e3adc5e63fdc19338e5bbdba622679696a57adbc0c9f32feab65637d9282dd8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=f32ff75e692d3fcf56601d7093ba396adf6ad2df5b7a6ff9025a79bd229b7e45\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=dc93f6a2eb30865a0e25357513d550a6abe44755b784cf8b2b2289ad355fd433\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=10ec086e41e988b9566eb9cfdd5e25ca83ed82fdb476415aedd352b2e5b447e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14346 sha256=204ae7a0427ab71cefb2b7d45392040f91c390f88defa89a8b03d488903ad80e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=053a22db65233d0f8b751721da1ddcd0064b496d13c9c2c52c32cf1afa7fddde\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built pinyin word2number docopt langdetect pptree sqlitedict wikipedia-api intervaltree\n",
            "Installing collected packages: word2number, sqlitedict, sortedcontainers, pptree, pinyin, docopt, terminaltables, semver, segtok, num2words, lru-dict, lemminflect, langdetect, jsonlines, jmespath, intervaltree, ftfy, conllu, anytree, wikipedia-api, OpenHowNet, language-tool-python, botocore, bioc, s3transfer, pytorch-revgrad, mpld3, boto3, bert-score, transformer-smaller-training-vocab, flair, textattack\n",
            "Successfully installed OpenHowNet-2.0 anytree-2.12.1 bert-score-0.3.13 bioc-2.1 boto3-1.35.58 botocore-1.35.58 conllu-4.5.3 docopt-0.6.2 flair-0.14.0 ftfy-6.3.1 intervaltree-3.1.0 jmespath-1.0.1 jsonlines-4.0.0 langdetect-1.0.9 language-tool-python-2.8.1 lemminflect-0.2.3 lru-dict-1.3.0 mpld3-0.5.10 num2words-0.5.13 pinyin-0.4.0 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.10.3 segtok-1.5.11 semver-3.0.2 sortedcontainers-2.4.0 sqlitedict-2.1.0 terminaltables-3.1.10 textattack-0.3.10 transformer-smaller-training-vocab-0.4.0 wikipedia-api-0.7.1 word2number-1.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Updating TextAttack package dependencies.\n",
            "textattack: Downloading NLTK required packages.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0d28dbb43534b14bd0266d066ef4c37"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "aIrD_agKalS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ef46c8-1186-47f2-84a0-4a25a8d23a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from textattack.models.wrappers import SklearnModelWrapper\n",
        "class MySklearnModelWrapper(SklearnModelWrapper):\n",
        "    def __call__(self, text_input_list, batch_size=32):\n",
        "        encoded_text_matrix = self.tokenizer.transform(text_input_list).toarray()\n",
        "        tokenized_text_df = pd.DataFrame(\n",
        "            encoded_text_matrix, columns=self.tokenizer.get_feature_names_out() # Use get_feature_names_out() here\n",
        "        )\n",
        "        return self.model.predict_proba(tokenized_text_df)\n",
        "with open(\"tfidf_vect_fit2.pkl\", 'rb') as file:\n",
        "    tokenizer = pickle.load(file)\n",
        "\n",
        "with open(\"tfidf_lr_model2.pkl\", 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "model_wrapper = MySklearnModelWrapper(model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBn5q7WCalS7",
        "outputId": "022c11d5-08b6-4b67-d9fd-84c46d44f7f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
            "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.targeted_classification.TargetedClassification'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack(\n",
            "  (search_method): GreedyWordSwapWIR(\n",
            "    (wir_method):  delete\n",
            "  )\n",
            "  (goal_function):  TargetedClassification(\n",
            "    (target_class):  0\n",
            "  )\n",
            "  (transformation):  WordSwapEmbedding(\n",
            "    (max_candidates):  50\n",
            "    (embedding):  WordEmbedding\n",
            "  )\n",
            "  (constraints): \n",
            "    (0): WordEmbeddingDistance(\n",
            "        (embedding):  WordEmbedding\n",
            "        (min_cos_sim):  0.5\n",
            "        (cased):  False\n",
            "        (include_unknown_words):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (1): PartOfSpeech(\n",
            "        (tagger_type):  nltk\n",
            "        (tagset):  universal\n",
            "        (allow_verb_noun_swap):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (2): UniversalSentenceEncoder(\n",
            "        (metric):  angular\n",
            "        (threshold):  0.840845057\n",
            "        (window_size):  15\n",
            "        (skip_text_shorter_than_window):  True\n",
            "        (compare_against_original):  False\n",
            "      )\n",
            "    (3): RepeatModification\n",
            "    (4): StopwordModification\n",
            "    (5): InputColumnModification(\n",
            "        (matching_column_labels):  ['premise', 'hypothesis']\n",
            "        (columns_to_ignore):  {'premise'}\n",
            "      )\n",
            "  (is_black_box):  True\n",
            ") \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 1 / 1:  10%|█         | 1/10 [00:00<00:00, 58.77it/s]\u001b[A\n",
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 2 / 2:  20%|██        | 2/10 [00:00<00:00, 61.98it/s]\u001b[A\n",
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 3 / 3:  30%|███       | 3/10 [00:00<00:00, 63.56it/s]\u001b[A\n",
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 4 / 4:  40%|████      | 4/10 [00:00<00:00, 62.03it/s]\u001b[A\n",
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 5 / 5:  50%|█████     | 5/10 [00:00<00:00, 61.99it/s]\u001b[A\n",
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 6 / 6:  60%|██████    | 6/10 [00:00<00:00, 64.63it/s]\u001b[A\n",
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 6 / 6:  70%|███████   | 7/10 [00:00<00:00, 66.10it/s]\u001b[A\n",
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 7 / 7:  70%|███████   | 7/10 [00:00<00:00, 63.35it/s]\u001b[A\n",
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 8 / 8:  80%|████████  | 8/10 [00:00<00:00, 66.24it/s]\u001b[A\n",
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 9 / 9:  90%|█████████ | 9/10 [00:00<00:00, 65.40it/s]\u001b[A\n",
            "[Succeeded / Failed / Skipped / Total] 0 / 0 / 10 / 10: 100%|██████████| 10/10 [00:00<00:00, 64.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Text Matrix Shape: (1, 100)\n",
            "Number of Features: 100\n",
            "--------------------------------------------- Result 1 ---------------------------------------------\n",
            "[[0 (36%)]] --> [[[SKIPPED]]]\n",
            "\n",
            " verification code from alabama group mim eversion contents ty le tex ht ml chars e tut f content transfer encoding bit mc task id mci p group online templet id content length antivirus vast vs inbound message x antivirus status cleaner james blue you are confirming login please enter the following code please pay attention after verification\n",
            "\n",
            "\n",
            "Encoded Text Matrix Shape: (1, 100)\n",
            "Number of Features: 100\n",
            "--------------------------------------------- Result 2 ---------------------------------------------\n",
            "[[0 (1%)]] --> [[[SKIPPED]]]\n",
            "\n",
            " wet pussy request come and make my holes derive never been faked properly can you give me favor and fulfill my dream nice ginger hair and big bubbly boobs do you think my books big enough bye cute\n",
            "\n",
            "\n",
            "Encoded Text Matrix Shape: (1, 100)\n",
            "Number of Features: 100\n",
            "--------------------------------------------- Result 3 ---------------------------------------------\n",
            "[[0 (5%)]] --> [[[SKIPPED]]]\n",
            "\n",
            " make me sweat can fc k all night long check up what i wrote toyo u bill\n",
            "\n",
            "\n",
            "Encoded Text Matrix Shape: (1, 100)\n",
            "Number of Features: 100\n",
            "--------------------------------------------- Result 4 ---------------------------------------------\n",
            "[[0 (100%)]] --> [[[SKIPPED]]]\n",
            "\n",
            " you have a problem with your papal account account limiteds tart shopping faster by adding a payment method email customer pay pal secure now check the account informations that belongs to youth is my account access will be limited your account access will be limited for the following reasons september we need to confirm some of your account information your case id for this reason is pp we face a problem in the ratification of the real owner of\n",
            "\n",
            "\n",
            "Encoded Text Matrix Shape: (1, 100)\n",
            "Number of Features: 100\n",
            "--------------------------------------------- Result 5 ---------------------------------------------\n",
            "[[0 (100%)]] --> [[[SKIPPED]]]\n",
            "\n",
            " your account has been closed nat west dear customer this email is confidential and intended for t headdress ee only please delete if that is not you tube to a recent security issue your account temporarily deactivated you are required to reactivate your online banking account in less than hours after your activation is completed you can manage your money pay bills transfer money and set up standing orders online yuan also search and print statements remember\n",
            "\n",
            "\n",
            "Encoded Text Matrix Shape: (1, 100)\n",
            "Number of Features: 100\n",
            "--------------------------------------------- Result 6 ---------------------------------------------\n",
            "[[0 (100%)]] --> [[[SKIPPED]]]\n",
            "\n",
            " important message from the support your account will be limited randoms trin g randoms trin g randoms trin g pay pal confirmation request confirm your papal wallets ome malicious bots have been flooding our networks with micro transactions lately therefore were asking our users confirm their accounts failure to verify this account in hours starting from date may lead to definitive account lockout another loss fall your balances tart the verification process herein you receive this e\n",
            "\n",
            "\n",
            "Encoded Text Matrix Shape: (1, 100)\n",
            "Number of Features: 100\n",
            "--------------------------------------------- Result 7 ---------------------------------------------\n",
            "[[0 (100%)]] --> [[[SKIPPED]]]\n",
            "\n",
            " bank of america final notification updater epl yt o dear client this is your official notification thatched service s listed below will be deactivated undeleted your profile is not verified immediately previous notifications was sent to the billing contact assigned your account as the primary contact you must renew yo urban k of america online banking reinstate thank you bank of america na member f dfc equal housing lender bank of america corporation all rights rese\n",
            "\n",
            "\n",
            "Encoded Text Matrix Shape: (1, 100)\n",
            "Number of Features: 100\n",
            "--------------------------------------------- Result 8 ---------------------------------------------\n",
            "[[0 (87%)]] --> [[[SKIPPED]]]\n",
            "\n",
            " your account access is temporarily limited return path envelope email uk delivery date sat nov received from port hel o server v nbe rco m by server web server net with est exit envelope from idi q shh hj for email sat nov received from port rhe louse r by server v nbe rco m with esp a exit envelope from id iqs frat tf sat nov reply\n",
            "\n",
            "\n",
            "Encoded Text Matrix Shape: (1, 100)\n",
            "Number of Features: 100\n",
            "--------------------------------------------- Result 9 ---------------------------------------------\n",
            "[[0 (100%)]] --> [[[SKIPPED]]]\n",
            "\n",
            " new security ale r turgent confirmation of online banking details dear chase online customers part of our commitment to help keep your accounts e cure we have detected an irregular activity on your account andre are placing a hold on your account for your protection please click here and follow the instructions to unlock your account we are here to assist you anytime your accounts ecu rit y is our priority thank you for choosing chase sincerely chase fraud\n",
            "\n",
            "\n",
            "Encoded Text Matrix Shape: (1, 100)\n",
            "Number of Features: 100\n",
            "--------------------------------------------- Result 10 ---------------------------------------------\n",
            "[[0 (16%)]] --> [[[SKIPPED]]]\n",
            "\n",
            " an individual has recently marked your current profile gi a has recently supplied a photograph and tagged yourself to see it view photograph\n",
            "\n",
            "\n",
            "\n",
            "+-------------------------------+------+\n",
            "| Attack Results                |      |\n",
            "+-------------------------------+------+\n",
            "| Number of successful attacks: | 0    |\n",
            "| Number of failed attacks:     | 0    |\n",
            "| Number of skipped attacks:    | 10   |\n",
            "| Original accuracy:            | 0.0% |\n",
            "| Accuracy under attack:        | 0.0% |\n",
            "| Attack success rate:          | 0%   |\n",
            "| Average perturbed word %:     | nan% |\n",
            "| Average num. words per input: | 59.1 |\n",
            "| Avg num queries:              | nan  |\n",
            "+-------------------------------+------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/textattack/metrics/attack_metrics/words_perturbed.py:83: RuntimeWarning: Mean of empty slice.\n",
            "  average_perc_words_perturbed = self.perturbed_word_percentages.mean()\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/textattack/metrics/attack_metrics/attack_queries.py:39: RuntimeWarning: Mean of empty slice.\n",
            "  avg_num_queries = self.num_queries.mean()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7dac1cff8130>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7dac1cff8040>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7dac1cff9ae0>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7dac24aece50>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7dac1cff8670>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7dac1cff8af0>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7dac1cff8970>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7dac1cff8370>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7dac1cffa260>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7dac1cff9120>]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "from textattack.datasets import HuggingFaceDataset\n",
        "from textattack.goal_functions import TargetedClassification\n",
        "from textattack.attack_recipes import TextFoolerJin2019, DeepWordBugGao2018\n",
        "from textattack import Attacker\n",
        "import textattack\n",
        "from textattack.datasets import Dataset\n",
        "\n",
        "test_data = pd.read_csv(\"Dataset2_2Test.csv\")\n",
        "dataset = Dataset([(row['final_cleaned_text'], row['label']) for _, row in test_data.iterrows()])\n",
        "attack = TextFoolerJin2019.build(model_wrapper)\n",
        "attack.goal_function = TargetedClassification(model_wrapper, target_class=0)\n",
        "\n",
        "attacker = Attacker(attack, dataset)\n",
        "attacker.attack_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v1RGsPyalS7"
      },
      "source": [
        "### Conclusion\n",
        "We were able to train a model on the IMDB dataset using `sklearn` and use it in TextAttack by initializing with the `SklearnModelWrapper`. It's that simple!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Example_1_sklearn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c0d28dbb43534b14bd0266d066ef4c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d505aaaeaf3e48ae89b34af78f3a8aa8",
              "IPY_MODEL_4727a90f5f774c5f8ee2ea5fa8b5264c",
              "IPY_MODEL_32350fc818ee4c2faee527102b1323b7"
            ],
            "layout": "IPY_MODEL_d86569714c18449084caac742ad93e53"
          }
        },
        "d505aaaeaf3e48ae89b34af78f3a8aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48dce1ef9f9d43a3b35bd6b084bcab78",
            "placeholder": "​",
            "style": "IPY_MODEL_ea0f7d2cb5884284b25ba77da9faf28c",
            "value": ""
          }
        },
        "4727a90f5f774c5f8ee2ea5fa8b5264c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f58e7362e45940459ca2be4bb7c63035",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62ad3bf5c77a4545b55c34f9296b0fb5",
            "value": 0
          }
        },
        "32350fc818ee4c2faee527102b1323b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_187e7c2335b94e1f9af230cef74798e9",
            "placeholder": "​",
            "style": "IPY_MODEL_064cdedbfd5944ffa2cf5a2fcfbc92ce",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "d86569714c18449084caac742ad93e53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48dce1ef9f9d43a3b35bd6b084bcab78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea0f7d2cb5884284b25ba77da9faf28c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f58e7362e45940459ca2be4bb7c63035": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "62ad3bf5c77a4545b55c34f9296b0fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "187e7c2335b94e1f9af230cef74798e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "064cdedbfd5944ffa2cf5a2fcfbc92ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}